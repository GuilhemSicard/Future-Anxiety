{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e69f5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import spacy\n",
    "from spacy.tokens import DocBin, Doc\n",
    "Doc.set_extension(\"ID\", default='')\n",
    "Doc.set_extension(\"headline\", default='')\n",
    "Doc.set_extension(\"label\", default='')\n",
    "import pandas as pd \n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "re.DEFAULT_VERSION = re.VERSION1\n",
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "import time\n",
    "import gc\n",
    "from bertopic import BERTopic \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import zipfile\n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f201c51",
   "metadata": {},
   "source": [
    "## Apply NLP pipeline for tokenization, lemmatization and other features for latter uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a582764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>pageNum</th>\n",
       "      <th>paragraphNum</th>\n",
       "      <th>content</th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Admbûstrateur . AUGUSTE DUNIONT ABOMMBHXins Pa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>m part ri pas demain à l'occasion du jour de l...</td>\n",
       "      <td>Le Figaro</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>20-21-22-23-24</td>\n",
       "      <td>Notre éclectisme en roli tique qui, pour être ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>25-26</td>\n",
       "      <td>fœil bravement fixé sur le couteau, gravi les ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>27-28</td>\n",
       "      <td>. Hier matii , lés gens dé Batignolles considé...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791868</th>\n",
       "      <td>1910-12-31</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>Le sujet imaginé par MM. Gheusi et Mé rane aur...</td>\n",
       "      <td>LA SOIRÉE LE MIRACLE A L'OPÉRA</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791869</th>\n",
       "      <td>1910-12-31</td>\n",
       "      <td>5</td>\n",
       "      <td>9-10-11-12-13-14-15</td>\n",
       "      <td>M lle Chenal M. Muratore si l'on en juge parla...</td>\n",
       "      <td>A L'OPÉRA Le Miracle</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791870</th>\n",
       "      <td>1910-12-31</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>à 8 heures pour les représentations de M. Gili...</td>\n",
       "      <td>Ce-soir : A l'Opéra,</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791871</th>\n",
       "      <td>1910-12-31</td>\n",
       "      <td>5</td>\n",
       "      <td>18-19</td>\n",
       "      <td>l'Habitation forcée SUITE derrière le vert ble...</td>\n",
       "      <td>Feuilleton du FIGARO du 31 Décembre</td>\n",
       "      <td>autres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791872</th>\n",
       "      <td>1910-12-31</td>\n",
       "      <td>5</td>\n",
       "      <td>20-21-22-23-24</td>\n",
       "      <td>montrer le chien, le nez tout blanc de lait so...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>791873 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  pageNum         paragraphNum  \\\n",
       "0       1870-01-01        0                    9   \n",
       "1       1870-01-01        0                   13   \n",
       "2       1870-01-01        0       20-21-22-23-24   \n",
       "3       1870-01-01        0                25-26   \n",
       "4       1870-01-01        0                27-28   \n",
       "...            ...      ...                  ...   \n",
       "791868  1910-12-31        5                    7   \n",
       "791869  1910-12-31        5  9-10-11-12-13-14-15   \n",
       "791870  1910-12-31        5                   16   \n",
       "791871  1910-12-31        5                18-19   \n",
       "791872  1910-12-31        5       20-21-22-23-24   \n",
       "\n",
       "                                                  content  \\\n",
       "0       Admbûstrateur . AUGUSTE DUNIONT ABOMMBHXins Pa...   \n",
       "1       m part ri pas demain à l'occasion du jour de l...   \n",
       "2       Notre éclectisme en roli tique qui, pour être ...   \n",
       "3       fœil bravement fixé sur le couteau, gravi les ...   \n",
       "4       . Hier matii , lés gens dé Batignolles considé...   \n",
       "...                                                   ...   \n",
       "791868  Le sujet imaginé par MM. Gheusi et Mé rane aur...   \n",
       "791869  M lle Chenal M. Muratore si l'on en juge parla...   \n",
       "791870  à 8 heures pour les représentations de M. Gili...   \n",
       "791871  l'Habitation forcée SUITE derrière le vert ble...   \n",
       "791872  montrer le chien, le nez tout blanc de lait so...   \n",
       "\n",
       "                                   headline    label  \n",
       "0                                                     \n",
       "1                                 Le Figaro           \n",
       "2                                                     \n",
       "3                                                     \n",
       "4                                                     \n",
       "...                                     ...      ...  \n",
       "791868       LA SOIRÉE LE MIRACLE A L'OPÉRA  culture  \n",
       "791869                 A L'OPÉRA Le Miracle  culture  \n",
       "791870                 Ce-soir : A l'Opéra,  culture  \n",
       "791871  Feuilleton du FIGARO du 31 Décembre   autres  \n",
       "791872                                                \n",
       "\n",
       "[791873 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textDf = pd.read_csv('data/le_figaro.csv')\n",
    "textDf = textDf.fillna('')\n",
    "textDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b24343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████▎               | 391872/791873 [1:42:38<1:44:46, 63.63it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_bin = DocBin(store_user_data=True)\n",
    "\n",
    "nlp = spacy.load('fr_core_news_lg', exclude=[\"ner\"])\n",
    "texts = ((row.content, ('_'.join([str(row['date']),str(row['pageNum']),str(row['paragraphNum'])]), \n",
    "                        row['headline'], row['label']))\n",
    "         for _, row in textDf.iterrows())\n",
    "count = 0\n",
    "nDocs = len(textDf)\n",
    "with tqdm(total=len(textDf),mininterval = 5, miniters =1000) as pbar:\n",
    "    for doc, (ID, headline, label) in nlp.pipe(texts,as_tuples = True, batch_size=2048,n_process=16):\n",
    "        doc._.ID = ID\n",
    "        doc._.headline = headline\n",
    "        doc._.label = label\n",
    "        doc_bin.add(doc)\n",
    "        pbar.update(1)\n",
    "        #split data into several docbin\n",
    "        if (count%50_000 == 0) or (count == nDocs-1):\n",
    "            i = int(count/50_000)-4 if (count != nDocs-1) else int(count/50_000) -3\n",
    "            if count>0:\n",
    "                doc_bin.to_disk(f\"data/spacy/le_figaro{i}.spacy\")\n",
    "                del doc_bin\n",
    "                gc.collect()\n",
    "            doc_bin = DocBin(store_user_data=True)\n",
    "            \n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b060d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488202d8",
   "metadata": {},
   "source": [
    "# Pachinko allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191e37cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [01:34<00:00,  7.88s/it]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('fr_core_news_lg', disable=[\"ner\"])\n",
    "\n",
    "doc_bin = DocBin(store_user_data=True)\n",
    "for root, dirs, files in os.walk('data/spacy'):\n",
    "    for name in tqdm(files):\n",
    "        if name.endswith((\".spacy\")):\n",
    "            doc_bin.merge(DocBin(store_user_data=True).from_disk(\"./data/spacy/\"+name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a336f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(doc):\n",
    "    ID = doc._.ID\n",
    "    headline = doc._.headline\n",
    "    label = doc._.label\n",
    "    oovRatio = np.mean([1 if word.is_oov else 0 for word in doc if word.is_alpha])\n",
    "    lemmas = [word.lemma_.lower() for word in doc \n",
    "           if word.is_alpha and (not word.is_stop) and (len(word.lemma_)>2) and (not word.is_oov)] \n",
    "    \n",
    "    return ID, doc.text, lemmas, oovRatio, headline, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d2a31b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709265\n"
     ]
    }
   ],
   "source": [
    "docs = [preprocess_text(doc) for doc in doc_bin.get_docs(nlp.vocab)]\n",
    "textDf = pd.DataFrame(docs, columns=['ID','content','lemmatized','oovRatio','headline','label'])\n",
    "textDf = textDf[(textDf.lemmatized.apply(len)>5) &\n",
    "                (textDf.oovRatio<0.6) &\n",
    "                (textDf.label=='')]\n",
    "print(len(textDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7d53f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_articles = textDf[\"lemmatized\"].tolist()\n",
    "bigram = Phrases(raw_articles, min_count=10, threshold=10)\n",
    "bigram_mod = Phraser(bigram)\n",
    "raw_articles = list(bigram_mod[raw_articles])\n",
    "trigram = Phrases(raw_articles, min_count=10, threshold=10)\n",
    "trigram_mod = Phraser(bigram)\n",
    "raw_articles = list(trigram_mod[raw_articles])\n",
    "textDf[\"nGram\"] = raw_articles\n",
    "textDf[\"nGram\"] = textDf[\"nGram\"].apply(' '.join)\n",
    "textDf[\"lemmatized\"] = textDf[\"lemmatized\"].apply(' '.join)\n",
    "textDf.to_csv('data/le_figaro_lemmatized_without_stop.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be7a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "textDf = pd.read_csv('data/le_figaro_lemmatized_without_stop.csv')\n",
    "raw_articles = textDf.nGram.str.split().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df3cf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = tp.utils.Corpus()\n",
    "for doc in raw_articles:\n",
    "    if doc:\n",
    "        corpus.add_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be86ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1_max = 7\n",
    "k1_min = 2 \n",
    "k2_max = 18\n",
    "k2_min = 8\n",
    "params = []\n",
    "scores = []\n",
    "num_iter = 0\n",
    "max_iter = 0\n",
    "for k1 in range(k1_min, k1_max):\n",
    "    for k2 in range(max(k1,k2_min),k2_max):\n",
    "        max_iter +=1\n",
    "        \n",
    "start = time.time()\n",
    "for k1 in range(k1_min, k1_max):\n",
    "    for k2 in range(max(k1,k2_min),k2_max):\n",
    "        num_iter+=1\n",
    "        model = tp.PAModel(tw=tp.TermWeight.IDF,min_cf = 10, min_df=5, rm_top=25, k1=k1, k2=k2, corpus=corpus)\n",
    "        model.burn_in = 20\n",
    "        model.train(40, workers=24)\n",
    "        score = tp.coherence.Coherence(model, coherence=\"c_v\").get_score()\n",
    "        params.append((k1,k2))\n",
    "        scores.append(score)\n",
    "        print(\"Runtime: %.2f seconds\" %(time.time() - start), \"|| Number of Searches: %s out of  %s\" %(num_iter, max_iter), \"|| k1: %s & k2: %s || coherence : %.3f\" %(k1,k2,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d5d1a2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k1: 3 & k2: 9 || coherence : 0.659\n",
      "k1: 4 & k2: 11 || coherence : 0.685\n",
      "k1: 4 & k2: 14 || coherence : 0.694\n",
      "k1: 5 & k2: 15 || coherence : 0.668\n"
     ]
    }
   ],
   "source": [
    "for ind,score in enumerate(scores):\n",
    "    if score>max(scores)*0.95:\n",
    "        print(\"k1: %s & k2: %s || coherence : %.3f\" %(params[ind][0],params[ind][1],score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dfebf28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7261144071817398\n"
     ]
    }
   ],
   "source": [
    "k1 = 4\n",
    "k2 = 14\n",
    "model = tp.PAModel(tw=tp.TermWeight.IDF,min_cf = 10, min_df=5, rm_top=25, k1=k1, k2=k2, corpus=corpus)\n",
    "model.burn_in = 20\n",
    "model.train(100, workers=24)\n",
    "score = tp.coherence.Coherence(model, coherence=\"c_v\").get_score()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2fb35589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0\n",
      "\t ['gouvernement', 'angleterre', 'anglais', 'allemagne', 'ministre', 'russie', 'france', 'français', 'allemand', 'empereur']\n",
      "Topic #1\n",
      "\t ['officier', 'armée', 'troupe', 'soldat', 'capitaine', 'militaire', 'navire', 'commandant', 'guerre', 'colonel']\n",
      "Topic #2\n",
      "\t ['comte', 'mlle', 'comtesse', 'comte_comtesse', 'marquis', 'baron', 'vicomte', 'baronne', 'paul', 'docteur']\n",
      "Topic #3\n",
      "\t ['politique', 'france', 'pays', 'république', 'peuple', 'chose', 'esprit', 'gouvernement', 'estper', 'idée']\n",
      "Topic #4\n",
      "\t ['monsieur', 'femme', 'aller', 'oeil', 'dire', 'rien', 'main', 'cœur', 'vie', 'jamais']\n",
      "Topic #5\n",
      "\t ['rue', 'maison', 'eau', 'vendre', 'prix', 'franc', 'blanc', 'vin', 'noir', 'gramme']\n",
      "Topic #6\n",
      "\t ['prix', 'roi', 'course', 'fête', 'prince', 'cheval', 'palais', 'voiture', 'empereur', 'majesté']\n",
      "Topic #7\n",
      "\t ['théâtre', 'représentation', 'pièce', 'artiste', 'opéra', 'jouer', 'mlle', 'soir', 'succès', 'rôle']\n",
      "Topic #8\n",
      "\t ['franc', 'chambre', 'commission', 'député', 'loi', 'séance', 'million', 'budget', 'voter', 'projet']\n",
      "Topic #9\n",
      "\t ['rue', 'arrêter', 'femme', 'hier', 'âgé', 'police', 'agent', 'individu', 'maison', 'ouvrier']\n",
      "Topic #10\n",
      "\t ['gouvernement', 'président', 'loi', 'chambre', 'journal', 'contre', 'lettre', 'ministre', 'affaire', 'droit']\n",
      "Topic #11\n",
      "\t ['président', 'église', 'abbé', 'société', 'lieu', 'mgr', 'maire', 'évêque', 'docteur', 'nommer']\n",
      "Topic #12\n",
      "\t ['art', 'œuvre', 'livre', 'artiste', 'tableau', 'portrait', 'exposition', 'peintre', 'roman', 'volume']\n",
      "Topic #13\n",
      "\t ['monsieur', 'femme', 'dire', 'enfant', 'aller', 'père', 'bon', 'rien', 'mari', 'ami']\n"
     ]
    }
   ],
   "source": [
    "for k in range(k2):\n",
    "    print('Topic #{}'.format(k))\n",
    "    print(\"\\t\", [w for w, p in model.get_topic_words(k)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a3532a",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "19fcdec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''## tokenizer ici meme, découper pour doc entre 256 et 512 bert token, en respectant les phrases et fournir embeddings + token + text\n",
    "def splitLongDoc(doc):\n",
    "    if len(doc._.label) != 0:\n",
    "        return '','', ''\n",
    "    length = len(doc)\n",
    "    if length>=800:\n",
    "        nSegment = int(length/800)+1\n",
    "        maxLen = length/nSegment\n",
    "        content = ['']\n",
    "        for sentence in doc.sents:\n",
    "            sentLen = len(sentence)\n",
    "            if (len(content[-1])+sentLen<maxLen) or (len(content[-1]) < 200):\n",
    "                content[-1] = content[-1] +' '+ sentence.text\n",
    "            else:\n",
    "                content.append(sentence.text)\n",
    "        if len(content[-1])<100:\n",
    "            content[-2] = content[-2] + ' ' +content[-1]\n",
    "            content = content[:-1]\n",
    "            \n",
    "    else:\n",
    "        text = doc.text\n",
    "        content = [text] if len(text)>100 else ''\n",
    "        \n",
    "    return doc._.ID, content, doc._.headline'''\n",
    "\n",
    "class DocSplitor:\n",
    "    def __init__(self, model = None, tokenizer = None, lim = 382, device='cuda:1'):\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        print(f'Computation on {model.device}')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lim = lim\n",
    "    \n",
    "    def mergeList(self, l1,l2):\n",
    "        return l1+l2\n",
    "    \n",
    "    def mergeStr(self, s1,s2):\n",
    "        return s1+' '+s2\n",
    "    \n",
    "    def merge(self, toks1, toks2):\n",
    "        return {'content':self.mergeStr(toks1['content'],toks2['content']), \n",
    "                'tokens':self.mergeList(toks1['tokens'],toks2['tokens']), \n",
    "                'lemmas':self.mergeList(toks1['lemmas'],toks2['lemmas']),\n",
    "                'nTokens':toks1['nTokens']+toks2['nTokens']}\n",
    "    \n",
    "    def mergeAll(self, tokenized):\n",
    "        if len(tokenized)>2:\n",
    "            return self.merge(tokenized[0],self.mergeAll(tokenized[1:]))\n",
    "        else:\n",
    "            return self.merge(*tokenized) if len(tokenized)>1 else tokenized[0]\n",
    "    \n",
    "    def checkLim(self, toks1,toks2):\n",
    "        return ((toks1['nTokens'] + toks2['nTokens']) <= self.lim)\n",
    "    \n",
    "    \n",
    "    def resize(self, sentence): \n",
    "        tokens = self.tokenizer.encode(sentence.text)[1:-1]\n",
    "        length = len(tokens)\n",
    "        ratio = len(tokens)/self.lim\n",
    "        if (ratio<=1): \n",
    "            segments = [(sentence,tokens)]\n",
    "        else:\n",
    "            segmentSize = int(length/(int(ratio)+2))+1\n",
    "            segments = [sentence[segmentSize*i:segmentSize*(i+1)] for i in range((int(ratio)+1))]\n",
    "            segments  =[(segment,self.tokenizer.encode(segment.text)[1:-1])\n",
    "                        for segment in segments]\n",
    "                         \n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    \n",
    "    def sentenceProcessing(self, sentence, tokens):\n",
    "        lemmas = [word.lemma_.lower() for word in sentence \n",
    "                  if word.is_alpha and (not word.is_stop) and (len(word.lemma_)>2) and (not word.is_oov)]\n",
    "        return {'content': sentence.text,\n",
    "                'tokens': tokens,\n",
    "                'lemmas': lemmas,\n",
    "                'nTokens': len(tokens)}\n",
    "    \n",
    "    \n",
    "    def groupSentences(self, tokens, length):\n",
    "        # function to merge list such that it's stay smaller than lim and each block is approximatly the same size \n",
    "        if (length>=3):\n",
    "            if (tokens[0]['nTokens'] <= tokens[2]['nTokens']) and self.checkLim(tokens[0],tokens[1]):\n",
    "                return ([self.merge(tokens[0],tokens[1])] + \n",
    "                        self.groupSentences(tokens[2:],length-2 ))\n",
    "            \n",
    "            elif self.checkLim(tokens[1],tokens[2]):\n",
    "                return ([tokens[0], self.merge(tokens[1],tokens[2])] + \n",
    "                        self.groupSentences(tokens[3:],length-3 ))\n",
    "            \n",
    "            else: \n",
    "                return (tokens[:2] +\n",
    "                        self.groupSentences(tokens[2:],length-2 ))\n",
    "            \n",
    "        elif (length==2):\n",
    "            if self.checkLim(tokens[0],tokens[1]):\n",
    "                return [self.merge(tokens[0],tokens[1])] \n",
    "            \n",
    "            else:\n",
    "                return tokens\n",
    "            \n",
    "        else:\n",
    "            return tokens\n",
    "\n",
    "    def tokenize(self, doc):\n",
    "\n",
    "        if len(doc._.label) != 0 or (len(doc)<50):\n",
    "            return ('','',[], [], [], [])\n",
    "        # [1:-1] to remove delimiter\n",
    "        \n",
    "        tokenized = [self.sentenceProcessing(segment,tokens) \n",
    "                     for sentence in doc.sents\n",
    "                     for (segment,tokens) in self.resize(sentence)]\n",
    "        if sum([sentence['nTokens'] for sentence in tokenized])>self.lim:\n",
    "            \n",
    "            mergeable = True\n",
    "            while mergeable:\n",
    "                nBlock = len(tokenized)\n",
    "                tokenized = self.groupSentences(tokenized,nBlock)\n",
    "                mergeable = not (nBlock == len(tokenized))\n",
    "        else: \n",
    "            tokenized = [self.mergeAll(tokenized)]\n",
    "\n",
    "        return (doc._.ID, \n",
    "                doc._.headline, \n",
    "                [str(i) for i in range(len(tokenized))],\n",
    "                [sentence['content'] for sentence in tokenized], \n",
    "                [sentence['lemmas'] for sentence in tokenized], \n",
    "                [[2]+sentence['tokens']+[3] for sentence in tokenized])\n",
    "    \n",
    "    def getAttentionMask(self, attentionLen): \n",
    "        return [1]*attentionLen + [0]*(self.lim+2-attentionLen)\n",
    "    \n",
    "    \n",
    "    def batchEmbeds(self, input_ids,attention_mask, tokEmb = False, nToks = None):\n",
    "        \n",
    "        output = self.model(input_ids=input_ids.to(self.device),\n",
    "                            attention_mask=attention_mask.to(self.device),\n",
    "                            output_hidden_states=tokEmb,\n",
    "                            output_attentions=False)\n",
    "        if tokEmb:\n",
    "            tEmbs = sum(output.hidden_states[-4:])/4\n",
    "            tEmbs = torch.vstack([tEmbs[ind,:attentionLen,:] \n",
    "                                  for ind, attentionLen in enumerate(nToks)])\n",
    "            return tEmbs.cpu().numpy().astype('float16')\n",
    "        \n",
    "        else:\n",
    "            return output.pooler_output.cpu().numpy().astype('float16')\n",
    "    \n",
    "    \n",
    "    def saveNumpyAsZip(self, embs,index,path,nbatch):\n",
    "        \n",
    "        if os.path.isfile(path):\n",
    "            with zipfile.ZipFile(path, 'a') as archive:\n",
    "                with archive.open(f'index{nbatch}.npy','w') as file:\n",
    "                    np.save(file, index)\n",
    "                with archive.open(f'embs{nbatch}.npy','w') as file:\n",
    "                    np.save(file, embs)\n",
    "        else: \n",
    "            with open(path, 'wb') as file:\n",
    "                np.savez_compressed(file,**{ f'index{nbatch}' :index , f'embs{nbatch}' : embs})\n",
    "                \n",
    "                \n",
    "    def getEmbeds(self, tokens, ID, tokEmb = False):\n",
    "        nToks= [len(toks) for toks in tokens]\n",
    "        attention_mask = torch.ByteTensor([self.getAttentionMask(attentionLen)\n",
    "                                          for attentionLen in nToks],device='cpu')\n",
    "        \n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence([torch.IntTensor(toks,device='cpu') \n",
    "                                                     for toks in [[0]*(self.lim+2)]+tokens], \n",
    "                                                     batch_first=True)[1:,:]\n",
    "\n",
    "        if tokEmb:\n",
    "            self.getTokEmbeds(input_ids, attention_mask, ID, nToks, batchSize=256)\n",
    "        else:\n",
    "            self.getDocEmbeds(input_ids, attention_mask, ID, batchSize=512)\n",
    "        \n",
    "    \n",
    "    def getDocEmbeds(self, input_ids, attention_mask, ID, saveFreq =100, batchSize = 512):\n",
    "        docEmbs = []\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(range(0,len(ID),batchSize)):\n",
    "                \n",
    "                dEmbs = self.batchEmbeds(input_ids[batch:batch+batchSize,:],\n",
    "                                         attention_mask[batch:batch+batchSize,:])\n",
    "                \n",
    "                docEmbs.append(dEmbs)\n",
    "                \n",
    "                if ((batch%(batchSize*saveFreq)== 0) and(batch>0))  or (batch+batchSize>=len(ID)):\n",
    "                    \n",
    "                    docInd = np.array(ID[batchSize*saveFreq*count:batchSize*saveFreq*(count+1)])\n",
    "                    self.saveNumpyAsZip(np.vstack(docEmbs),docInd,'data/bertEmbeddings/docEmbs.npz',count)\n",
    "                    del docEmbs, docInd\n",
    "                    docEmbs = []\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    count+=1\n",
    "                    \n",
    "                    \n",
    "    def getTokEmbeds(self, input_ids, attention_mask, ID, nToks, saveFreq =1000, batchSize = 256):\n",
    "        tokEmbs =  []\n",
    "        count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(range(0,len(ID),batchSize)):\n",
    "                tEmbs = self.batchEmbeds(input_ids[batch:batch+batchSize,:],\n",
    "                                         attention_mask[batch:batch+batchSize,:],\n",
    "                                         tokEmb = True,\n",
    "                                         nToks = nToks[batch:batch+batchSize])\n",
    "                \n",
    "                tokEmbs.append(tEmbs)\n",
    "                \n",
    "                if ((batch%(batchSize*saveFreq)== 0) and(batch>0))  or (batch+batchSize>=len(ID)):\n",
    "                    \n",
    "                    tokInd = np.array([(ID[batchSize*saveFreq*count+i],tokIndex) \n",
    "                                       for i, attentionLen \n",
    "                                       in enumerate(nToks[batchSize*saveFreq*count:batchSize*saveFreq*(count+1)])\n",
    "                                       for tokIndex in range(attentionLen)])\n",
    "                    #self.tokInd = tokInd\n",
    "                    #self.tokEmbs = tokEmbs\n",
    "                    self.saveNumpyAsZip(np.vstack(tokEmbs),tokInd,'data/bertEmbeddings/tokEmbs.npz',count)\n",
    "                    del tokEmbs, tokInd\n",
    "                    tokEmbs =  []\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    count+=1\n",
    "                    \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "19578831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>headline</th>\n",
       "      <th>segmentID</th>\n",
       "      <th>content</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1898-03-21_0_27_0</td>\n",
       "      <td>De notre correspondant de</td>\n",
       "      <td>0</td>\n",
       "      <td>Budapest : A l'occasion du prochain jubilé du ...</td>\n",
       "      <td>[budapest, occasion, prochain, jubilé, règne, ...</td>\n",
       "      <td>[2, 16466, 30, 37, 79, 11, 2213, 378, 2086, 71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1898-03-21_0_29_0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Au Tribunal correctionnel : X Le président . E...</td>\n",
       "      <td>[tribunal, correctionnel, président, avoir, vo...</td>\n",
       "      <td>[2, 748, 11746, 13330, 30, 60, 447, 1031, 18, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1898-03-21_0_30_0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>. Nous aurons le plaisir d'offrir demain à nos...</td>\n",
       "      <td>[avoir, plaisir, offrir, demain, lecteur, prim...</td>\n",
       "      <td>[2, 18, 970, 10172, 354, 3906, 71, 11, 7002, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1898-03-21_0_31_0</td>\n",
       "      <td>POUR ALICE LA VIGNE AVEUGLE -</td>\n",
       "      <td>0</td>\n",
       "      <td>Nous ne publierons que d tnain la quatrième li...</td>\n",
       "      <td>[publier, liste, souscription, recevoir, repré...</td>\n",
       "      <td>[2, 970, 446, 7212, 406, 393, 71, 6477, 468, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1898-03-21_0_32-33_0</td>\n",
       "      <td>De notre correspondant de</td>\n",
       "      <td>0</td>\n",
       "      <td>Nice : t Ce matin, deux services ont-été céléb...</td>\n",
       "      <td>[nice, matin, service, être, célébrer, neuf, h...</td>\n",
       "      <td>[2, 5759, 30, 87, 860, 974, 16, 603, 3186, 507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791865</th>\n",
       "      <td>1878-05-13_2_9-10-11-12-13-14-15_11</td>\n",
       "      <td>PREMIÈRES KEFEÉSEHTATIONS</td>\n",
       "      <td>11</td>\n",
       "      <td>-soir dimanche à la représentation clés Sept C...</td>\n",
       "      <td>[soir, dimanche, représentation, clé, château,...</td>\n",
       "      <td>[2, 17, 897, 1993, 130, 348, 4079, 25010, 1242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791866</th>\n",
       "      <td>1878-05-13_2_17-18_0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>coursés au bois de boulogne Les journées se su...</td>\n",
       "      <td>[bois, boulogne, journée, longchamps, brillant...</td>\n",
       "      <td>[2, 1009, 381, 366, 2590, 336, 5502, 2293, 519...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791866</th>\n",
       "      <td>1878-05-13_2_17-18_1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Ce costume est très à-la mode. Il m'a paru aus...</td>\n",
       "      <td>[costume, mode, paraître, chapeau, fort, genre...</td>\n",
       "      <td>[2, 860, 10300, 401, 739, 130, 17, 348, 4726, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791866</th>\n",
       "      <td>1878-05-13_2_17-18_2</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>Mais nous ne manquons pas de courses en semain...</td>\n",
       "      <td>[manquer, course, semaine, pouvoir, rattraper,...</td>\n",
       "      <td>[2, 811, 512, 446, 774, 19943, 435, 336, 4843,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791866</th>\n",
       "      <td>1878-05-13_2_17-18_3</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>Stathouder ne sera pas loin du vainqueur du Jo...</td>\n",
       "      <td>[stathouder, loin, vainqueur, jockey, club, su...</td>\n",
       "      <td>[2, 27613, 8933, 3869, 446, 866, 435, 2414, 37...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1369388 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ID                       headline  \\\n",
       "0                         1898-03-21_0_27_0      De notre correspondant de   \n",
       "2                         1898-03-21_0_29_0                                  \n",
       "3                         1898-03-21_0_30_0                                  \n",
       "4                         1898-03-21_0_31_0  POUR ALICE LA VIGNE AVEUGLE -   \n",
       "5                      1898-03-21_0_32-33_0      De notre correspondant de   \n",
       "...                                     ...                            ...   \n",
       "791865  1878-05-13_2_9-10-11-12-13-14-15_11      PREMIÈRES KEFEÉSEHTATIONS   \n",
       "791866                 1878-05-13_2_17-18_0                                  \n",
       "791866                 1878-05-13_2_17-18_1                                  \n",
       "791866                 1878-05-13_2_17-18_2                                  \n",
       "791866                 1878-05-13_2_17-18_3                                  \n",
       "\n",
       "       segmentID                                            content  \\\n",
       "0              0  Budapest : A l'occasion du prochain jubilé du ...   \n",
       "2              0  Au Tribunal correctionnel : X Le président . E...   \n",
       "3              0  . Nous aurons le plaisir d'offrir demain à nos...   \n",
       "4              0  Nous ne publierons que d tnain la quatrième li...   \n",
       "5              0  Nice : t Ce matin, deux services ont-été céléb...   \n",
       "...          ...                                                ...   \n",
       "791865        11  -soir dimanche à la représentation clés Sept C...   \n",
       "791866         0  coursés au bois de boulogne Les journées se su...   \n",
       "791866         1  Ce costume est très à-la mode. Il m'a paru aus...   \n",
       "791866         2  Mais nous ne manquons pas de courses en semain...   \n",
       "791866         3  Stathouder ne sera pas loin du vainqueur du Jo...   \n",
       "\n",
       "                                                   lemmas  \\\n",
       "0       [budapest, occasion, prochain, jubilé, règne, ...   \n",
       "2       [tribunal, correctionnel, président, avoir, vo...   \n",
       "3       [avoir, plaisir, offrir, demain, lecteur, prim...   \n",
       "4       [publier, liste, souscription, recevoir, repré...   \n",
       "5       [nice, matin, service, être, célébrer, neuf, h...   \n",
       "...                                                   ...   \n",
       "791865  [soir, dimanche, représentation, clé, château,...   \n",
       "791866  [bois, boulogne, journée, longchamps, brillant...   \n",
       "791866  [costume, mode, paraître, chapeau, fort, genre...   \n",
       "791866  [manquer, course, semaine, pouvoir, rattraper,...   \n",
       "791866  [stathouder, loin, vainqueur, jockey, club, su...   \n",
       "\n",
       "                                                   tokens  \n",
       "0       [2, 16466, 30, 37, 79, 11, 2213, 378, 2086, 71...  \n",
       "2       [2, 748, 11746, 13330, 30, 60, 447, 1031, 18, ...  \n",
       "3       [2, 18, 970, 10172, 354, 3906, 71, 11, 7002, 2...  \n",
       "4       [2, 970, 446, 7212, 406, 393, 71, 6477, 468, 3...  \n",
       "5       [2, 5759, 30, 87, 860, 974, 16, 603, 3186, 507...  \n",
       "...                                                   ...  \n",
       "791865  [2, 17, 897, 1993, 130, 348, 4079, 25010, 1242...  \n",
       "791866  [2, 1009, 381, 366, 2590, 336, 5502, 2293, 519...  \n",
       "791866  [2, 860, 10300, 401, 739, 130, 17, 348, 4726, ...  \n",
       "791866  [2, 811, 512, 446, 774, 19943, 435, 336, 4843,...  \n",
       "791866  [2, 27613, 8933, 3869, 446, 866, 435, 2414, 37...  \n",
       "\n",
       "[1369388 rows x 6 columns]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()['_361']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f33e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-french-europeana-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n",
    "model =  AutoModel.from_pretrained('dbmdz/bert-base-french-europeana-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e678fdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation on cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitor = DocSplitor(model,tokenizer)\n",
    "            \n",
    "docs = [splitor.tokenize(doc) for doc in doc_bin.get_docs(nlp.vocab)]\n",
    "BERTDf = pd.DataFrame(docs, columns=['ID','headline','segmentID','content','lemmas','tokens'])\n",
    "BERTDf = BERTDf.explode(['segmentID','content','lemmas','tokens'])\n",
    "BERTDf  =BERTDf[~BERTDf.content.isna()].copy()\n",
    "BERTDf['ID'] += '_' +BERTDf['segmentID'] \n",
    "BERTDf.to_csv('data/le_figaro_BERT.csv',index=False)\n",
    "\n",
    "del docs\n",
    "del doc_bin\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "2615a69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "512\n",
      "[256, 256]\n"
     ]
    }
   ],
   "source": [
    "#with open('data/bertEmbeddings/embs.npz', 'wb') as file:\n",
    "#    np.savez_compressed(file, **{'ok':[1,2,3],'wah':[3,2,1]})\n",
    "    \n",
    "with open(f'data/bertEmbeddings/tokEmbs.npz', 'rb') as tokFile:\n",
    "    test = np.load(tokFile)\n",
    "    #ind = pd.DataFrame((test[f'index{0}'][0]))\n",
    "    #embs = pd.DataFrame(test[f'embs{0}']).apply(np.array,axis=1).rename(\"embs\")\n",
    "    content = [pd.concat([pd.DataFrame((test[f'index{i}']),columns = ['ID','tokInd']),\n",
    "                          pd.DataFrame(test[f'embs{i}']).apply(np.array,axis=1).rename(\"embs\")],axis=1) \n",
    "               for i in range(int(len(test.files)/2))]\n",
    "    print(len(content))\n",
    "    print(len(set(content[0].ID.values)|set(content[1].ID.values)))\n",
    "    print([len(np.unique(d.ID)) for d in content])\n",
    "    df  = pd.concat(content)\n",
    "        \n",
    "        #ind, embs = zip(*[(ind, embs) for ind, embs in test.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da1c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "6836d72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 2675/2675 [11:00:31<00:00, 14.82s/it]\n"
     ]
    }
   ],
   "source": [
    "splitor = DocSplitor(model,tokenizer,device='cuda:0')\n",
    "#test = BERTDf.sample(512)\n",
    "splitor.getEmbeds(BERTDf.tokens.tolist(),BERTDf.ID.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "c3382801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ZipFile.__del__ at 0x7fd84e986dc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/lib/python3.9/zipfile.py\", line 1816, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/anaconda/lib/python3.9/zipfile.py\", line 1833, in close\n",
      "    self.fp.seek(self.start_dir)\n",
      "ValueError: seek of closed file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del splitor\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "b8f9989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation on cuda:0\n"
     ]
    }
   ],
   "source": [
    "topic = {'science':['découverte','science','scienti','techni'],\n",
    "         'futur': ['déclin','décadence','futur','avenir','progrès']}\n",
    "keywords = topic['science'] + topic['futur']\n",
    "\n",
    "BERTDfKws = BERTDf[BERTDf.content.apply(unidecode).str.lower().apply(lambda x : any(kw in x for kw in keywords))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb572c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation on cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████████████████▉                        | 145/352 [18:16<25:52,  7.50s/it]"
     ]
    }
   ],
   "source": [
    "splitor = DocSplitor(model,tokenizer,device='cuda:1')\n",
    "splitor.getEmbeds(BERTDfKws.tokens.tolist(),BERTDfKws.ID.tolist(), tokEmb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e178abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/bertEmbeddings/docEmbs.npz', 'rb') as tokFile:\n",
    "    zipedFiles = np.load(tokFile)\n",
    "    #remain = [pd.DataFrame(zipedFile[f'embs{i}'][len(zipedFile[f'index{i}']):])\n",
    "    #           for i in range(int(len(zipedFile.files)/2))]\n",
    "    #content = [pd.DataFrame(zipedFile[f'embs{i}'][:len(zipedFile[f'index{i}'])], index= zipedFile[f'index{i}'])\n",
    "    #           for i in range(int(len(zipedFile.files)/2))]\n",
    "    embs = [zipedFiles[file][:len(zipedFiles[file])]\n",
    "            for file in zipedFiles if 'embs' in file]\n",
    "    index = [zipedFiles[file][:len(zipedFiles[file])]\n",
    "            for file in zipedFiles if 'index' in file]\n",
    "    docEmbs = pd.DataFrame(np.vstack(embs),index =np.concatenate(index)).apply(np.array,axis=1)\n",
    "    del embs\n",
    "    del index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "82670536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed038ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   max_df = 0.7,\n",
    "                                   min_df = 10,\n",
    "                                   strip_accents = 'unicode')\n",
    "topic_model = BERTopic(verbose=True,\n",
    "                       #embedding_model=embedding_model,#'dbmdz/bert-base-french-europeana-cased',\n",
    "                       nr_topics = 'auto',\n",
    "                       min_topic_size = 100,\n",
    "                       vectorizer_model = vectorizer_model)\n",
    "topics, probs = topic_model.fit_transform(BERTDf.set_index('ID').loc[docEmbs.index].content.tolist(),\n",
    "                                          np.array(docEmbs.tolist()))\n",
    "topic_model.save(\"BERTopic.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97d05fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 768)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test['docEmbs'].tolist()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7de7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "a = tokenizer(texts[:20])\n",
    "print(time.time()-t)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eea5402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb8da65352b4fbc86f5b063bb214ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbc53ad74c0448dbdd46d218411683a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b3f4eef15847de8b674f808f84bafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/222k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-french-europeana-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "decee43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1898-03-22_0_7</td>\n",
       "      <td>H. DE VILLEMESSANï? Fondateur ABONNEMENT ' Tro...</td>\n",
       "      <td>On t'abonne dans tous les Bureaux de Poste de ...</td>\n",
       "      <td>[2, 639, 87, 11, 8467, 422, 671, 361, 11881, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1898-03-22_0_9</td>\n",
       "      <td>RÉDACTION ADMINISTRATION PUBUOTÏÎ 26, Rue Drou...</td>\n",
       "      <td>ré, i. UAIie 102.46 Rédaction TÉLÉPHONE 102.47...</td>\n",
       "      <td>[2, 437, 16, 76, 18, 57, 1913, 364, 3530, 18, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1898-03-26_1_31</td>\n",
       "      <td></td>\n",
       "      <td>Notre Service' de Librairie so chargé d'envoye...</td>\n",
       "      <td>[2, 2997, 9323, 11, 336, 14730, 573, 3252, 71,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1898-03-26_3_95</td>\n",
       "      <td>pOLE j ORD pOLE j jORD</td>\n",
       "      <td>Ouvert de 8 h du matin à midi minuit. Salle en...</td>\n",
       "      <td>[2, 7760, 336, 28, 75, 378, 974, 130, 1486, 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>1898-04-01_3_21</td>\n",
       "      <td>Ce-soir, au Gymnase,</td>\n",
       "      <td>neuvième spectacle d'abonnement, 4 série des v...</td>\n",
       "      <td>[2, 14855, 3953, 71, 11, 9769, 16, 24, 1567, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733307</th>\n",
       "      <td>1878-05-11_2_56</td>\n",
       "      <td>A. DALIFOL, 172, quai Jemmapes, 172 FORGES et ...</td>\n",
       "      <td>Mach à vapeur ver tic' 1 et horizont 1 144, Fa...</td>\n",
       "      <td>[2, 19208, 130, 5283, 1127, 5239, 11, 21, 353,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733326</th>\n",
       "      <td>1878-05-11_2_98</td>\n",
       "      <td>FARINE MORTON ,</td>\n",
       "      <td>Alimentation naturelle des Enfants. Se vend ch...</td>\n",
       "      <td>[2, 29816, 7697, 370, 14532, 18, 1083, 2733, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733377</th>\n",
       "      <td>1878-05-11_2_196</td>\n",
       "      <td></td>\n",
       "      <td>Mme R désire trouver place de dame de compagni...</td>\n",
       "      <td>[2, 868, 54, 4735, 2061, 1087, 336, 4211, 336,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733485</th>\n",
       "      <td>1878-05-12_2_100</td>\n",
       "      <td>APP grands et petits, chamb</td>\n",
       "      <td>cabin magas boutiq terrains il louer. Pris mod...</td>\n",
       "      <td>[2, 23585, 213, 4402, 2705, 18214, 9362, 399, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733511</th>\n",
       "      <td>1878-05-12_2_144</td>\n",
       "      <td>HOTELS RECOMMANDÉS MAISON E. LAUER, rue de Cho...</td>\n",
       "      <td>app meublés, Restaurant des Familles: déjeuner...</td>\n",
       "      <td>[2, 546, 23766, 16, 21206, 370, 18452, 212, 30...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4509 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ID                                           headline  \\\n",
       "44        1898-03-22_0_7  H. DE VILLEMESSANï? Fondateur ABONNEMENT ' Tro...   \n",
       "45        1898-03-22_0_9  RÉDACTION ADMINISTRATION PUBUOTÏÎ 26, Rue Drou...   \n",
       "354      1898-03-26_1_31                                                      \n",
       "397      1898-03-26_3_95                             pOLE j ORD pOLE j jORD   \n",
       "777      1898-04-01_3_21                               Ce-soir, au Gymnase,   \n",
       "...                  ...                                                ...   \n",
       "733307   1878-05-11_2_56  A. DALIFOL, 172, quai Jemmapes, 172 FORGES et ...   \n",
       "733326   1878-05-11_2_98                                    FARINE MORTON ,   \n",
       "733377  1878-05-11_2_196                                                      \n",
       "733485  1878-05-12_2_100                        APP grands et petits, chamb   \n",
       "733511  1878-05-12_2_144  HOTELS RECOMMANDÉS MAISON E. LAUER, rue de Cho...   \n",
       "\n",
       "                                                  content  \\\n",
       "44      On t'abonne dans tous les Bureaux de Poste de ...   \n",
       "45      ré, i. UAIie 102.46 Rédaction TÉLÉPHONE 102.47...   \n",
       "354     Notre Service' de Librairie so chargé d'envoye...   \n",
       "397     Ouvert de 8 h du matin à midi minuit. Salle en...   \n",
       "777     neuvième spectacle d'abonnement, 4 série des v...   \n",
       "...                                                   ...   \n",
       "733307  Mach à vapeur ver tic' 1 et horizont 1 144, Fa...   \n",
       "733326  Alimentation naturelle des Enfants. Se vend ch...   \n",
       "733377  Mme R désire trouver place de dame de compagni...   \n",
       "733485  cabin magas boutiq terrains il louer. Pris mod...   \n",
       "733511  app meublés, Restaurant des Familles: déjeuner...   \n",
       "\n",
       "                                                   tokens  \n",
       "44      [2, 639, 87, 11, 8467, 422, 671, 361, 11881, 3...  \n",
       "45      [2, 437, 16, 76, 18, 57, 1913, 364, 3530, 18, ...  \n",
       "354     [2, 2997, 9323, 11, 336, 14730, 573, 3252, 71,...  \n",
       "397     [2, 7760, 336, 28, 75, 378, 974, 130, 1486, 65...  \n",
       "777     [2, 14855, 3953, 71, 11, 9769, 16, 24, 1567, 3...  \n",
       "...                                                   ...  \n",
       "733307  [2, 19208, 130, 5283, 1127, 5239, 11, 21, 353,...  \n",
       "733326  [2, 29816, 7697, 370, 14532, 18, 1083, 2733, 1...  \n",
       "733377  [2, 868, 54, 4735, 2061, 1087, 336, 4211, 336,...  \n",
       "733485  [2, 23585, 213, 4402, 2705, 18214, 9362, 399, ...  \n",
       "733511  [2, 546, 23766, 16, 21206, 370, 18452, 212, 30...  \n",
       "\n",
       "[4509 rows x 4 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERTDf[BERTDf.tokens.apply(len)<20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5d9d9931",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_510521/1833530288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "a = model(torch.tensor([tokenizer.encode(t) for t in texts[:3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bc33e23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4733e-01,  1.6857e-01,  6.5338e-01,  1.7024e-01,  5.9312e-02,\n",
       "          2.0989e-02, -3.5540e-01,  3.5775e-01,  1.0148e+00,  6.4225e-01,\n",
       "         -1.5062e-01,  1.0290e+01, -2.5911e-01,  6.7702e-01, -1.9008e-01,\n",
       "         -6.8659e-02,  1.2729e+00,  1.0137e+00,  8.2785e-01, -4.8681e-01,\n",
       "          4.3917e-01,  3.0807e-01, -3.1741e-02, -1.6302e+00, -7.0986e-01,\n",
       "         -1.5057e-01,  3.5652e-01,  4.2652e-01, -1.2928e-01,  6.0701e-01,\n",
       "          1.6977e+00,  9.8092e-01, -4.3878e-01,  5.2851e-01, -1.0169e+00,\n",
       "         -1.4644e+00, -1.0988e+00,  4.5423e-01,  7.5249e-01,  1.4139e-01,\n",
       "          7.5322e-01, -1.3733e+00,  2.2834e-01,  1.5011e+00,  3.0981e-01,\n",
       "         -6.1100e-01,  8.8547e-01,  6.7765e-01,  1.3227e-01,  4.6886e-01,\n",
       "          1.1077e+00,  7.7351e-01,  2.5898e-01, -4.8556e-01,  5.0439e-01,\n",
       "         -9.4194e-01, -3.0323e-01,  2.6878e-01,  4.5885e-01, -1.0652e-01,\n",
       "         -3.8497e+00,  5.9461e-01,  6.5919e-01, -1.7917e-01,  3.8417e-01,\n",
       "         -3.3791e-01,  6.4720e-03,  1.3651e+00,  3.0746e-02, -6.8294e-01,\n",
       "         -1.3566e-02, -5.4125e-01,  1.0557e+00,  7.6070e-01, -9.5182e-01,\n",
       "         -1.7622e-01, -5.3862e-01,  7.2537e-01,  6.5018e-01, -4.8309e-01,\n",
       "          1.4612e+00,  3.4327e-01,  7.6196e-01, -1.1436e+00,  1.9382e-01,\n",
       "          5.8315e-01,  1.4030e-01,  9.9842e-01, -1.8908e-01,  1.4912e-01,\n",
       "         -1.2278e+00,  6.6154e-01, -3.1481e-01,  1.2873e+00,  2.3559e-01,\n",
       "          6.7026e-01,  4.3381e-01,  4.0306e-01,  5.3612e-01,  6.9303e-01,\n",
       "          1.0361e+00, -9.9359e-02,  3.5250e-04,  2.9944e-01, -1.8461e-01,\n",
       "         -1.1522e+00, -1.0399e+00,  6.3375e-01, -2.4813e-02, -1.0133e-01,\n",
       "          3.1047e-01, -8.4756e-01, -8.4701e-02, -1.9350e+00, -9.1229e-02,\n",
       "          1.0577e+00, -5.4043e-01, -2.2326e-01,  4.3882e-01,  4.1657e-01,\n",
       "          5.8048e-01, -1.4938e-01,  4.4916e-01,  3.5052e-01,  2.2111e-01,\n",
       "         -1.3802e+00,  9.6033e-02, -1.9509e-01,  1.1860e+00, -1.3745e-01,\n",
       "         -1.2132e-02, -4.0332e-01,  3.1923e-01,  3.5714e-01,  4.5496e-01,\n",
       "         -9.6581e-01, -9.9411e-02, -4.0692e-01, -1.1433e+00, -2.2485e-01,\n",
       "          3.3203e-01,  2.7777e-01,  1.6259e+00, -7.5806e-01, -3.2124e-01,\n",
       "          7.2341e-01,  2.0528e-01,  1.0726e-01,  8.3345e-01, -8.1179e-02,\n",
       "          9.7931e-01,  7.0208e-01,  1.4116e-01, -1.9627e-01,  1.5868e-01,\n",
       "         -9.8644e-02,  5.8408e-01,  2.7004e-01, -8.0131e-01, -7.5227e-01,\n",
       "          5.7379e-02, -2.1252e-01,  1.0427e-01,  3.3908e-01, -6.3522e-01,\n",
       "         -3.5222e-01, -1.0394e-01,  7.1927e-03,  4.2023e-03, -1.3591e+00,\n",
       "         -3.7302e-01, -1.1680e+00, -1.1018e-01, -5.9658e-01, -2.9350e-01,\n",
       "         -1.0681e+00,  4.5662e-01,  1.7298e+00,  5.8842e-01, -8.7433e-01,\n",
       "          6.9463e-01,  2.0422e-01, -6.6786e-01,  7.0236e-01,  1.9172e-01,\n",
       "         -4.5097e-01, -7.1265e-01,  2.6415e-01, -1.0371e+00,  1.3279e-01,\n",
       "          2.8252e-01,  3.2885e-01,  6.7851e-01,  1.0562e+00,  3.6012e-01,\n",
       "          5.0244e-02,  1.4662e-01,  7.5716e-01, -3.6018e-01, -6.7832e-01,\n",
       "         -1.2246e+00,  2.3437e-01, -4.8369e-01,  2.2953e-01, -1.1138e+00,\n",
       "          1.1515e+00, -6.7686e-01, -3.7936e-01,  5.2488e-01,  2.4225e+00,\n",
       "          1.8507e+00,  1.1342e-01, -2.9648e-01,  7.9256e-01, -9.3260e-01,\n",
       "          4.8398e-01,  1.8675e-01,  6.8801e-02, -2.1604e-01, -9.3685e-01,\n",
       "         -4.6786e-01, -2.0483e-02,  7.2053e-02,  1.0900e+00, -3.4552e-01,\n",
       "          5.0402e-02,  1.2981e-01,  6.2809e-02, -9.4105e-01, -2.4464e-01,\n",
       "          8.5299e-01, -1.1915e+00,  1.0297e+00,  1.1003e+00,  2.8839e-01,\n",
       "          1.6897e+00,  4.5872e-01,  3.2419e-02,  3.5318e-01,  8.7430e-02,\n",
       "         -7.0627e-01, -1.0985e+00,  5.3748e-01,  5.8227e-02,  8.1624e-01,\n",
       "         -6.8147e-01, -1.2445e+00, -3.6810e-01, -7.1400e-01, -5.3966e-01,\n",
       "          1.6322e+00, -3.2584e-01, -4.9958e-01, -1.5097e+00,  5.8764e-01,\n",
       "          1.6246e-01, -3.4531e-01, -2.7272e-01,  2.0642e-01, -1.4835e-01,\n",
       "          7.2707e-01,  8.3451e-01,  4.5215e-01, -7.2408e-01,  1.7248e-01,\n",
       "         -4.3210e-01,  1.1027e+00, -2.7408e-01,  5.6535e-01, -2.1323e-01,\n",
       "          5.8470e-01,  2.2688e-01,  1.2087e-01,  1.0691e+00, -6.1902e-01,\n",
       "         -1.7514e-01,  1.2218e-01, -2.3918e+00, -1.2194e-03, -3.9054e-01,\n",
       "         -1.3801e+00, -1.0042e+00,  3.1594e-02,  7.4759e-01,  3.9836e-01,\n",
       "         -2.8818e-02, -5.7981e-01,  4.0783e-01, -8.8581e-01, -5.5408e-01,\n",
       "          8.7851e-02,  1.2561e-01,  4.2856e-01, -1.3477e-01, -1.0726e+00,\n",
       "          6.1915e-02, -7.0112e-01, -4.7437e-01,  2.6966e-01, -4.0683e-01,\n",
       "          8.4228e-01, -2.5641e+00,  7.5123e-01,  1.1969e-01,  8.2133e-01,\n",
       "          8.9502e-01,  5.4424e-01,  2.0124e+00, -4.9357e-01, -6.5060e-01,\n",
       "          6.9401e-01,  4.4736e-01, -6.1623e-02,  1.8162e-01, -1.2759e-02,\n",
       "         -5.2508e-01, -3.4071e-02, -7.6099e-01,  1.4535e-01,  8.2336e-01,\n",
       "         -3.9117e-01, -2.1255e-01, -7.3382e-01, -6.5234e-01, -6.5237e-01,\n",
       "          1.6502e-01, -2.7592e-01, -4.7344e-01,  1.1719e+00,  5.0828e-01,\n",
       "         -3.2992e-01,  2.0190e-01,  7.5964e-02, -2.4656e-01, -1.0692e+00,\n",
       "         -4.4544e-01, -9.5336e-01, -8.5917e-01,  1.4447e+00, -2.5540e-01,\n",
       "         -1.2238e+00,  4.6005e-01,  4.3120e-01, -1.7096e-02,  7.6191e-01,\n",
       "         -8.2655e-02,  3.8263e-02, -8.7749e-01, -7.3483e-01,  1.6726e+00,\n",
       "         -1.4903e+00, -7.8243e-02, -3.3159e-01, -5.9660e-01, -1.1235e+00,\n",
       "         -4.8498e-01, -1.8859e-01,  3.0639e-01, -9.7295e-01, -5.4008e-02,\n",
       "         -2.6464e-01,  3.3122e-01, -1.2480e+00,  9.8152e-01,  2.4925e-01,\n",
       "          4.6922e-01,  8.2269e-01,  7.0853e-01, -5.7752e-01, -1.0435e-01,\n",
       "         -1.5462e-01,  1.5080e+00,  6.6541e-02,  3.0961e-01,  1.6619e+00,\n",
       "          4.7680e-01,  6.1117e-02,  1.0978e-01,  8.8454e-01,  3.0876e-01,\n",
       "         -5.2636e-01, -6.3948e-02, -3.3218e-01, -4.7034e-01,  5.5758e-01,\n",
       "          1.1485e+00,  7.8035e-01,  3.5178e-01,  3.5141e-01, -8.8916e-01,\n",
       "          4.3091e-02,  4.5638e-01, -1.3518e-01,  9.7333e-01,  3.2826e-01,\n",
       "         -4.0461e-01, -3.4033e-01, -3.6625e-01, -7.2640e-02, -5.1233e-02,\n",
       "          1.8449e-02, -1.4707e-01, -9.0732e-01,  5.0135e-01,  2.7283e-01,\n",
       "          1.7797e-01,  2.5583e-02,  2.9001e-02,  7.8632e-02,  3.0781e-01,\n",
       "         -4.9282e-02,  2.7540e-01,  1.3102e+00,  1.1154e+00,  4.6194e-01,\n",
       "         -9.4660e-01,  4.3617e-01, -5.6585e-01,  6.9297e-01,  4.3371e-01,\n",
       "          2.3293e-01,  9.7845e-02,  3.8767e-01,  8.8539e-01,  1.1568e-01,\n",
       "         -3.5664e-01,  4.1398e-01, -2.8043e-01, -4.8251e-01, -7.0285e-01,\n",
       "          4.7583e-01, -7.3421e-01, -7.3861e-02, -7.3823e-01, -2.1487e-01,\n",
       "          1.1358e-01,  7.1551e-01, -1.3740e-01,  8.9643e-02, -1.7859e-01,\n",
       "          3.7497e-01, -5.5601e-01,  1.1438e+00,  3.4984e-01,  3.2997e-01,\n",
       "          1.9199e+00, -5.6920e-01, -2.3912e-01,  4.1679e-01, -5.9505e-02,\n",
       "          1.1727e+00, -4.7290e-01,  9.6206e-01, -2.1011e-01,  5.1767e-02,\n",
       "         -2.9282e-01, -7.4351e-01, -4.4696e-01,  4.6786e-03, -1.1656e-01,\n",
       "         -8.4424e-01,  9.2403e-01, -9.5477e-01,  5.3429e-01, -1.8024e+00,\n",
       "         -2.7731e-01,  1.4221e-01, -2.5945e-01,  1.5508e+00,  1.1854e-02,\n",
       "         -2.4731e-03,  5.0989e-01, -1.1898e+00, -6.7230e-01,  1.4248e-01,\n",
       "          4.4392e-01, -2.6842e-01,  1.2849e-01, -1.6687e-01, -8.9063e-03,\n",
       "          1.1511e-02,  5.7928e-01, -7.9400e-02,  6.3430e-02, -1.7544e+00,\n",
       "          4.8020e-01,  3.2770e-01, -2.8784e-01,  5.8551e-01, -1.5439e+00,\n",
       "          1.7621e-01, -7.5941e-01, -3.2800e-02,  3.9727e-01, -1.0021e+00,\n",
       "          2.9242e-01,  7.4425e-01,  7.2250e-02, -4.3178e-02, -1.6224e-01,\n",
       "          9.8201e-01, -3.2882e-01,  1.2311e+00, -8.5904e-01, -9.9724e-02,\n",
       "         -2.2311e-01, -5.4871e-01,  2.0901e-02, -4.0401e-01, -3.6632e-01,\n",
       "          2.4051e-01, -2.8778e-01, -7.7238e-01,  4.8986e-01, -3.3492e-01,\n",
       "         -6.3014e-01,  5.5035e-01, -6.3238e-02,  8.3464e-01, -1.2838e+00,\n",
       "          1.0696e+00, -2.5362e-01,  5.4574e-02, -6.5876e-01, -1.5865e-01,\n",
       "         -2.6684e-02, -1.1846e+00, -1.0671e+00, -4.6253e-02,  6.7159e-01,\n",
       "          4.0318e-01, -7.0603e-01, -1.0807e+00,  2.6685e-01, -3.4085e-01,\n",
       "          7.6396e-01,  1.1115e-01, -4.5457e-02,  1.1628e+00, -2.3174e-01,\n",
       "         -3.1292e-02,  4.0122e-01,  8.1265e-01, -1.2599e+00, -3.4189e-01,\n",
       "          7.0385e-02, -9.1558e-01, -7.3630e-01,  6.4489e-01,  3.8275e-01,\n",
       "         -1.3202e-01, -2.0126e-02,  4.3461e-01,  1.8639e-01, -4.7580e-01,\n",
       "         -7.8056e-01, -7.6105e-01,  1.4139e-01,  1.9102e+00,  9.6066e-01,\n",
       "          8.1737e-01,  1.1612e+00,  1.5670e+00, -1.0549e+00, -6.2163e-02,\n",
       "         -3.7482e-01, -1.2209e-01,  2.6124e-01, -2.8383e-01,  6.5274e-01,\n",
       "          1.8056e+00,  4.9863e-01, -2.1568e-01, -1.0485e+00, -3.1742e-01,\n",
       "          8.0296e-01, -4.9856e-01, -3.5057e-01,  1.7652e-01,  1.8702e-01,\n",
       "         -1.7433e+00,  4.2791e-01, -5.3488e-02,  2.7068e-01, -4.8153e-01,\n",
       "         -4.8704e-02,  4.3360e-01, -1.1284e-01, -4.1310e-01,  8.4397e-01,\n",
       "         -4.9774e-01,  4.4864e-01, -3.1243e-01, -3.8142e-01,  7.3051e-01,\n",
       "         -1.2046e-01, -2.9051e-01, -9.9639e-01, -9.4109e-01,  2.4018e-01,\n",
       "         -2.3442e-02,  1.4337e+00,  1.0985e+00, -2.9353e-01,  1.5657e-01,\n",
       "         -1.5513e-01,  1.8167e+00, -6.3077e-01, -1.4371e+00, -2.6263e-01,\n",
       "          4.1983e-01,  1.2196e-01, -1.5802e-01,  1.0599e-01,  5.5723e-01,\n",
       "          4.4808e-01,  1.4131e+00,  7.8301e-01, -2.5851e-01, -4.4008e-01,\n",
       "         -1.2024e+00, -2.5321e-01,  2.2325e-01, -2.5295e-01,  2.1418e-01,\n",
       "          4.2256e-01, -1.1358e+00,  1.1895e-01, -4.8576e-01,  3.9331e-01,\n",
       "         -4.8658e-01,  7.2977e-01,  9.4750e-02,  1.7960e-01, -2.6666e-01,\n",
       "          8.1212e-01,  6.2561e-01,  1.7513e-01, -1.6294e-01,  2.9604e-01,\n",
       "          7.2869e-01, -2.7869e-01,  2.8107e-01,  1.6150e-01, -6.1578e-02,\n",
       "          1.0471e-01,  1.7592e-01,  5.6769e-01,  2.7749e-03, -1.3999e+00,\n",
       "          1.8404e-01, -6.7161e-01,  7.8181e-01,  4.2140e-01,  7.5353e-02,\n",
       "          1.4011e+00,  1.2287e-01,  4.7947e-01, -1.3361e-01,  1.4676e+00,\n",
       "         -1.7417e-01, -1.0354e+00, -2.8703e-01,  1.7474e-01,  2.6146e+00,\n",
       "          5.9831e-01,  4.1878e-01,  2.9461e-01,  1.1059e+00,  6.9503e-01,\n",
       "          4.6308e-01, -9.7399e-01,  8.6926e-02, -1.4387e+00, -1.7009e+00,\n",
       "          9.1589e-02, -7.2158e-01,  1.3908e+00, -3.0182e-01,  1.1405e+00,\n",
       "         -1.4967e-02,  8.4273e-02, -6.6411e-01,  2.5491e-01,  1.6888e+00,\n",
       "         -8.1042e-01, -3.6149e-01, -7.2188e-01, -5.2947e-01,  6.9994e-01,\n",
       "         -6.5983e-02,  9.8116e-01, -5.7392e-01,  4.8202e-01,  1.0063e+00,\n",
       "         -1.4472e-01, -2.0145e-01, -6.1907e-01,  5.5434e-02,  8.2694e-01,\n",
       "          7.2135e-01, -4.5967e-01,  2.5607e-01, -2.6761e-02,  1.0400e-01,\n",
       "         -4.1009e-01,  1.7160e-01,  3.4004e-01,  2.4915e+00,  3.3080e-01,\n",
       "         -1.3786e-01, -2.5761e-01, -4.2822e-01,  2.6949e-01,  9.6408e-03,\n",
       "          3.5802e-01, -2.6444e-01, -6.6891e-01, -1.5384e+00, -1.1359e-01,\n",
       "          2.5158e-01,  1.9122e-01,  2.4439e-01, -1.7134e+00,  1.4984e-01,\n",
       "          3.5242e-01,  1.6483e-01,  3.7138e-01, -4.6348e-01,  2.1692e-01,\n",
       "         -7.3519e-02,  3.8869e-01, -4.2166e-01, -5.9101e-01,  7.3236e-02,\n",
       "          4.1445e-01, -3.8036e-01,  4.1060e-01,  4.1485e-02,  5.3604e-01,\n",
       "          8.4443e-01, -8.5307e-01,  3.8064e-02, -6.8703e-02,  5.9325e-01,\n",
       "         -4.1067e-01, -2.4199e-01,  2.8542e-02,  3.2239e-01, -8.2968e-03,\n",
       "          8.4156e-01, -8.9558e-01, -6.9371e-01,  8.8933e-01, -6.4954e-01,\n",
       "         -2.3171e-01, -1.1262e+00, -5.4681e-01,  2.8653e-02,  3.6859e-01,\n",
       "          4.9379e-01, -8.1976e-01,  7.2943e-01,  1.9579e-01, -2.8676e-01,\n",
       "         -8.0873e-01,  1.2252e+00,  3.0236e-01]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['last_hidden_state'][:,0,:]-a['pooler_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2ee88cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_hidden_state', 'pooler_output']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0abbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
